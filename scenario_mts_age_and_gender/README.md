C url 6.3 итерации в секунде; без - 7.5

* Решать задачи как отедльные

---

Не обработаны: ['url_host', 'request_cnt' ]


---

1. Подготовка датасета
    1. Добавить `request_cnt`
        * Скорее всего, нужно добавить to_float
        * Число запросов одного пользователя за время дня (поле part_of_day)
        * Как принято обрабатывать авторами: в ptls.preprocessing из коробки есть два варианта: log_norm и identity_tranasform
        * request_cnt - это числа от 1 до 16 
        * Как нормализовали в google 2015? 
    2. Добаить `url_host`
2. Обучение 
    * Датасет весит больше 16 гб => для локального запуска нужно использовать iterable датасет

---
url_host - понять как выделять фичи
* разделить регуляркой по точке, по дефису и о цифрами

Можно сделать отдельную мапу домен -> эмбеддинг
Можно счиать их прямо в make_datasets_spark

1. выбросить домен верхнего уровня (ru / com)
1. Расставить пробелы, убрать ненужные символы. Предлагаю просто [a-zA-Z]+ то есть оставляем непрерываные последовательности букв
2. => bert
3. Транслитерировать => rubert
    * voronezhskaya -> воронежская
    * chto--proishodit-ru.turbopages.org что--проишодит-ру.турбопагес.орг
4. Можно банально усреднить эмбеддинги по времени; можно обучить колес
5. Можно сразу сделать pca / t-sne, чтобы посмотреть что получилось. Желательно какую-то нтервактивую верссию, чтобы виделть точки в пространсстве и при навежении было видно исхожный домен


6. xn--22-glcqfm3bya1b.xn--p1ai - так отображаются сайты, которые исходно на русском. Это, к слову, был грузчик22.рф. Такая кодировка называется punycode. Я научился декодировать ее в русский:

```python
def convert_punycode(puny_domen: str) -> str:
    """
    Converts punycode to unicode

    Example:
    >>> convert_punycode('xn--22-glcqfm3bya1b.xn--p1ai')
    <<< 'грузчик22.рф'
    """
    return puny_domen.encode().decode('idna')
```

All punycode domains start with 'xn--' and thus are easy to detect. I suppose that resular domains won't use such prefix:

```python
def is_punycode(s: str) -> bool:
    return s.startswith('xn--')
```

---


* Мы не умеем различать порядок множества кликов с одинаковым date и part_of_day. Однако, порядок может влиять на результат.
    * Либо рандомизировать на каждой эпохе обучения
    * Либо назначить строгий порядок при создании датасета


Кажется, что для нас все события, происходящие в одно время дня как бы одновременны: например, если пользователь несколько раз зашел на один сайт за один part_of_day, мы просто указываем число таких запросов в колонке `request_cnt`
















1) Пример как работает транслитерация + разбиение на слова:
krasnodar.avtotochki.ru
['краснодар', 'автоточки']

2) Небольшая проблема: часто бывает написано множество слов без разделителей:
koshkanasha.ru
['кошканаша']
smeshnoekino.ru
['смешноекино']
nizhnekamsk.ktogdeest.com
['нижнекамск', 'ктогдеест']
temavoronezh.ru
['темаворонеж']
vsesrazu.su
['всесразу']

Я не знаю как это обрабатывать, но надеюсь, что благодаря тому, что используются wordpiece токены, эмбеддинг последовательности не сильно пострадает

3) Проблема побольше: слова на английском встречаются примерно так же часто как и слова на русском. Найти мультиязычную модель - не проблема. Непонятно как определять перед нами транслитерированная кириллица, которую нужно транслитерировать обратно или корректное латинское слово. Была мысль просто завести для каджого домена два эмбеддинга: с трансслитерировацией и без. Но мне не кажется это хорошим решением


4) Удаление высокоуровневых доменов: Я всегда удаляю домен верхнего уровня (ru, com и тд). Возможно, нужно завести много правил и например, удалять некоторые домены второго уровня (lifejournal, turbopages). Например, можно просто удалить n самых частых доменов второго уровня. Возможно некоторые домны первого уровня не нужно удалять: например, иногда домен .net бывает осмысленным словом "нет". Хотя, я бы все равно удалял все домены первого уровня: описанная ситуация слишком редкая


5) Есть множество слов, которые сходу не получается транслитерировать: в частности все слова, содержащие букву x. Я думаю как раз их всегда считать латинскими и не траслитерируя подавать в модель при уловии, что она мультиязычная (у сберовского ruBert'а есть токены с латиницей в токенизаторе, но не знаю, насколько много было зарубежного текста в обучающем датасете)







получение слов из url сейчас не идеально:
* понижение размерности я пока не исопльзовал, хотя наверное нужно, а то у меня каждая фича кодируется эмбеддингом размером 4, а url - размером 256 
* сегментированные слова всегда транслитерируются без проверки являются ли они корректными русскими / англйскими словами
* если встречаются символы, которые не могут быть транслитерированы обратно в англйский они остаются такими как есть
* вообще транслитерация не инвертируемая операция, поэтому не всегда получаются корректные русские слова